# WEEK 5

[TOC]

# 训练神经网络

## 模型代码介绍

**SparseCategoricalCrossentropy**-稀疏分类交叉熵

![image-20221126194614902](C:\Users\41824\AppData\Roaming\Typora\typora-user-images\image-20221126194614902.png)

- Step 1 构造模型
- Step 2 定义代价函数-compile（编译）
- Step 3 拟合函数

## 训练细节

**specify**-指定，列入

![image-20221126195212239](C:\Users\41824\AppData\Roaming\Typora\typora-user-images\image-20221126195212239.png)

下面详细介绍神经网络的细节：**BinaryCrossentropy**-二元交叉熵函数

![image-20221126195514969](C:\Users\41824\AppData\Roaming\Typora\typora-user-images\image-20221126195514969.png)

可以调用[不同的损失函数](https://zhuanlan.zhihu.com/p/44216830)such as **MeanSquaredError**-平方差

实际上要求偏导时在函数中都帮你做到了。

<img src="C:\Users\41824\AppData\Roaming\Typora\typora-user-images\image-20221126201422170.png" style="zoom:67%;" />

（大佬们的库）

# 激活函数

## 之前讲过的一些激活函数

把很多很多的0.1函数写进整个模型：就像神经元一样。

ReLU函数-**Rectified Linear Unit**(矫正线性函数)

<img src="./assets/image-20221127100107817.png" alt="image-20221127100107817" style="zoom:67%;" />

<img src="./assets/image-20221127100251025.png" alt="image-20221127100251025" style="zoom:67%;" />

## 激活函数的选择

- 二元分类问题-**Sigmoid**
- 回归问题-**Linear activation functions**线性激活函数
- 非负回归问题-**ReLU**

![image-20221127100640505](./assets/image-20221127100640505.png)

往往**Sigmoid**会使得梯度下降变慢

![image-20221127101005469](./assets/image-20221127101005469.png)

![image-20221127101056061](./assets/image-20221127101056061.png)

每年都会有新的激活函数提出

## 激活函数的重要性

> 就是不要老去用线性回归，本末倒置了

![image-20221127101328629](./assets/image-20221127101328629.png)

像这样化成两层去线性回归，还不如直接用一个线性回归。

![image-20221127101714911](./assets/image-20221127101714911.png)

每层中都用线性回归，最后用sigmoid的话，结果出来跟逻辑回归没区别

![image-20221127101903558](./assets/image-20221127101903558.png)

# 多类问题-Multiclass Problem

## 多类问题介绍

![image-20221127102210756](./assets/image-20221127102210756.png)

![image-20221127102256519](./assets/image-20221127102256519.png)

> 个人想法：把后三类分成一类，然后一步一步二分类（比较慢）

## Softmax激活函数

**embelish**-美化（泛化，推广）

![image-20221127103035206](./assets/image-20221127103035206.png)

n=2的时候
$$
a_1 = \frac{e^{z_1}}{e^{z_1}+e^{z_2}} = \frac{1}{e^{z_1-z_2}+1}\\
a_2 = \frac{e^{z_2}}{e^{z_1}+e^{z_2}} = \frac{1}{e^{z_2-z_1}+1}
$$
所以参数会有些不一样（因为之前是$z$后面的是$z_1-z_2$，所以$z = z_1-z_2$，参数上也会发生相对的变动。

![image-20221127110400750](./assets/image-20221127110400750.png)

<img src="./assets/4f42bb586827c532200fdf93d83c4d3.jpg" alt="4f42bb586827c532200fdf93d83c4d3" style="zoom: 25%;" />

是等价的，但是数值上不一定是一样的，用ln函数只要是加大惩罚力度，让$a_j$达到接近1的情况（其实另一个会趋近于oo，主要还是加快迭代速度），以使得代价函数最小。

![image-20221127110821574](./assets/image-20221127110821574.png)

## softmax与神经网络

特有属性，比喻要把所有的z计算出来才可以算a

![image-20221127113627875](./assets/image-20221127113627875.png)

**SparseCategoricalCrossentropy**-稀疏类

![image-20221127113846549](./assets/image-20221127113846549.png)

## softmax改进代码

> 怎么更加精确的计算

![image-20221127114116507](./assets/image-20221127114116507.png)

表明直接的表达式会更加的精确

![image-20221127114437950](./assets/image-20221127114437950.png)把z记为中间量而不是a

![image-20221127114805773](./assets/image-20221127114805773.png)

![image-20221127115002492](./assets/image-20221127115002492.png)

对之前的逻辑回归问题

![image-20221127115027603](./assets/image-20221127115027603.png)

# 多标签分类问题-Multi-label classification

**pedestrain**-行人

![image-20221127115309322](./assets/image-20221127115309322.png)

![image-20221127115637450](./assets/image-20221127115637450.png)

- **多类型**与**多标签**的差异
- 多类型输出的是每一种值的可能性（0.1，0.2，0.7）

- 多标签输出的是每一个的取值（0，1，0）

# 进阶优化算法

之前的是一个固定的learning rate，现在，我们可以找到一个更加自动化的算法**Adam**

![image-20221127141808233](./assets/image-20221127141808233.png)

**Adaptive Moment estimation**算法

![image-20221127141937129](./assets/image-20221127141937129.png)

![image-20221127142026418](./assets/image-20221127142026418.png)

**Robust**-健壮（容错更高）

# 小拓展

**dense layer type**-连接层（后一项数据由前一项给出）

![image-20221127142428981](./assets/image-20221127142428981.png)

卷积层-**convolutional layer**，卷积神经网络-**CNN**（convolutional neural network）

![image-20221127144251009](./assets/image-20221127144251009.png)

目前一些其他的前沿模型：**Transformer**,**LSTM**(长短期记忆网络）,**attention**（注意力模型）



