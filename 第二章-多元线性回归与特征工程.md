# WEEK 2

<img align = "left" src="./assets/image-20221123201741188.png" alt="image-20221123201741188" style="zoom: 67%;" />



[TOC]

# 多特征向量（入门）-Multiple features

**vector**-向量

<img src="./assets/image-20221123203205812.png" alt="image-20221123203205812" style="zoom:67%;" />

当我们考虑多个影响时，可以想象下面的这个形式来表述

<img align="left" src="./assets/image-20221123203356992.png" alt="image-20221123203356992" style="zoom:50%;" />

**dot product**-点乘

<img src="./assets/image-20221123203902787.png" alt="image-20221123203902787" style="zoom: 50%;" />

# 多元线性回归-Multiple linear regression

## 矢量处理-向量运算

**vectorization**-矢量化，可以更容易处理你的数据

> Python的索引是从零开始的

- 手动计算点乘<img src="./assets/image-20221123204434599.png" alt="image-20221123204434599" style="zoom: 33%;" />

- 循环写法计算求和<img src="./assets/image-20221123204602412.png" alt="image-20221123204602412" style="zoom:33%;" />
- 向量的点乘<img src="./assets/image-20221123204659930.png" alt="image-20221123204659930" style="zoom:33%;" />

<img src="./assets/image-20221123204723148.png" alt="image-20221123204723148" style="zoom: 67%;" />

向量化可以更加方便运算与阅读，而且计算机的向量化是并行计算（硬件），相比于之前的会快很多。

<img src="./assets/image-20221123205109848.png" alt="image-20221123205109848" style="zoom:80%;" />

![image-20221123205337862](./assets/image-20221123205337862.png)

**向量化处理数据**-为了后续改善多元回归梯度下降

## 多元线性回归的梯度下降-Gradient descent for multiple linear regression

number-b，b不是向量，是个数。

<img src="./assets/image-20221123205845738.png" alt="image-20221123205845738" style="zoom:80%;" />
$$
对w求导\quad \frac{d}{dw_1}J(\vec w,b) = x_1^{(i)}\quad其余项都是常数项
$$
![image-20221123210239600](./assets/image-20221123210239600.png)

> **Normal equation**
>
> [详解正规方程（Normal Equation） - 知乎 ](https://zhuanlan.zhihu.com/p/60719445)
>
> （用到了很多的矩阵求导公式,可以用来检测结果）
>
> 优点：
>
> - 无需迭代
>
> 缺点：
>
> - 只能在多元线性回归中用到
>
> - 如果数据量过大，会很慢
>
>   <img align="left" src="./assets/image-20221123210655127.png" alt="image-20221123210655127" style="zoom:50%;" />

# 有关特征值与参数调节工程

## 特征放缩-Feature scaling

>  有的参数或者训练数据差异很大

<img src="./assets/image-20221124163019560.png" alt="image-20221124163019560" style="zoom:67%;" />

如何合理的取值（大小关系上）

<img src="./assets/image-20221124163226331.png" alt="image-20221124163226331" style="zoom:67%;" />

<img src="./assets/image-20221124163351938.png" alt="image-20221124163351938" style="zoom:67%;" />

缩放后更加精确美观的标出我们的取值范围区域

- 训练向量集的归一化-**Mean normalization**<img src="./assets/image-20221124163801674.png" alt="image-20221124163801674" style="zoom:50%;" />
- Z-score 标准化**Z-score normalization**<img src="./assets/image-20221124163954440.png" alt="image-20221124163954440" style="zoom:50%;" />

尽量使得所有的变量所属区间长度相近，特征放缩可以使梯度下降更快

<img align="left" src="./assets/image-20221124164625125.png" alt="image-20221124164625125" style="zoom:67%;" />

## 检验收敛性-Check the convergence

### 怎么看收敛性

一般都是要单减，否则说明有bug或者$\alpha$选的不好

<img src="./assets/image-20221124165359209.png" alt="image-20221124165359209" style="zoom:67%;" />

> 自动算法像是规定一个误差，减少基本趋于0时跳出。

### 如何选取$\alpha$

![image-20221124165833409](./assets/image-20221124165833409.png)

多带入一些$\alpha$的值进行测试（调参）

<img src="./assets/image-20221124170005112.png" alt="image-20221124170005112" style="zoom:67%;" />

## 特征工程-Feature Engineering

<img src="./assets/image-20221124170740687.png" alt="image-20221124170740687" style="zoom:67%;" />

用知识构造一个新的特征（变量）能够反映更多的特性，还可以使得整体更加拟合。

# 多项式回归-Polynomial Regression

<img src="./assets/image-20221124171128156.png" alt="image-20221124171128156" style="zoom:67%;" />

此时，特征放缩-**feature scaling**尤为重要。那么该如何选，以后的课程会分析各种情况下的优缺点。

